{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import re\n",
    "import treetaggerwrapper\n",
    "from os import path\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# нормализация запроса\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "\n",
    "def analyze_query(query):\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    tokens = word_tokenize(query)\n",
    "    punct = ',.()\":;--&?!\\'s'\n",
    "    tokens = [token for token in tokens if token not in punct]\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    new_query = ''\n",
    "    for token in tokens:\n",
    "        lemma = tagger.tag_text(token)[0].split('\\t')[-1]\n",
    "        new_query += lemma + ' '\n",
    "    \n",
    "    return new_query.lower().strip()\n",
    "\n",
    "# топики для моделей\n",
    "new_topics = json.loads(open('analyzed_topics.json').read())\n",
    "# модель с графом\n",
    "G = nx.read_gexf('topics.gexf')\n",
    "\n",
    "def sim_graph(word, results_n=10):   \n",
    "    found_topics = []\n",
    "    results = []\n",
    "    recs = set()\n",
    "    \n",
    "    for topic in new_topics:\n",
    "        n = re.search('(\\s|^)' + word.lower() + '(\\s|$)', topic)\n",
    "        if n is not None:\n",
    "            for node_1 in new_topics[topic]:\n",
    "                for node_2 in G.neighbors(node_1):\n",
    "                    if node_2 not in recs:\n",
    "                        n = re.search('(\\s|^|\\W)' + word.lower() + '(\\s|$|\\W)', node_2.lower())\n",
    "                        if n is None:\n",
    "                            recs.add(node_2)\n",
    "                            weight = G.edge[node_1][node_2]['weight']\n",
    "                            results.append([node_2, weight])\n",
    "\n",
    "    if results != []:\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)[:results_n]\n",
    "        results = [x[0] for x in results]\n",
    "\n",
    "    return results\n",
    "\n",
    "# модель word2vec\n",
    "model = json.loads(open('model_in_dic.json', 'r').read())\n",
    "def sim_word2vec(word, results_n=10):\n",
    "    pos_tags = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB']\n",
    "    results = []\n",
    "\n",
    "    for pos_tag in pos_tags:\n",
    "        word_tagged = word.replace(' ', '::') + '_' + pos_tag\n",
    "        if word_tagged in model:\n",
    "            for sim_word in model[word_tagged]:\n",
    "                sim_word = sim_word.replace('::', '(-| )')\n",
    "                for topic in new_topics:\n",
    "                    n = re.search('(\\s|^)' + sim_word.lower() + '(\\s|$)', topic)\n",
    "                    if n is not None and word not in topic:\n",
    "                        result = new_topics[topic] \n",
    "                        for r in result:\n",
    "                            if r not in results:\n",
    "                                 results.append(r)\n",
    "                            if len(results) >= results_n:\n",
    "                                return results\n",
    "\n",
    "    return results\n",
    "\n",
    "# модель wordnet\n",
    "def add_topics(lemma, word, results, results_n):\n",
    "    for topic in new_topics:\n",
    "        n = re.search('(\\s|^)' + lemma.replace('_', ' ') + '(\\s|$)', topic)\n",
    "        if n is not None:\n",
    "            m = re.search('(\\s|^)' + word.replace('_', ' ') + '(\\s|$)', topic)\n",
    "            if m is None:\n",
    "                for i in new_topics[topic]:\n",
    "                    results.add(i)\n",
    "                if len(results) >= results_n:\n",
    "                    return results\n",
    "    return results\n",
    "\n",
    "def sim_wordnet(word, results_n=10):\n",
    "    results = set()\n",
    "    word = word.replace(' ', '_')\n",
    "    lemmas = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for synonym in syn.lemmas():\n",
    "            synonym_name = synonym.name().lower()\n",
    "            if synonym_name != word and synonym_name not in lemmas:\n",
    "                lemmas.add(synonym_name)\n",
    "                results = add_topics(synonym_name, word, results, results_n)\n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "        \n",
    "    lemmas = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for synonym in syn.lemmas():\n",
    "            if synonym.antonyms():\n",
    "                for antonym in synonym.antonyms():\n",
    "                    antonym_name = antonym.name().lower()\n",
    "                    if antonym_name not in lemmas:\n",
    "                        lemmas.add(antonym_name)\n",
    "                        results = add_topics(antonym_name, word, results, results_n)\n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "    \n",
    "    lemmas = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.hypernyms():\n",
    "            for hyper in syn.hypernyms():\n",
    "                for hypernym in hyper.lemmas():\n",
    "                    hypernym_name = hypernym.name().lower()\n",
    "                    if hypernym_name not in lemmas:\n",
    "                        lemmas.add(hypernym_name)\n",
    "                        results = add_topics(hypernym_name, word, results, results_n)\n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "        \n",
    "    lemmas = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.hyponyms():\n",
    "            for hypo in syn.hyponyms():\n",
    "                for hyponym in hypo.lemmas():\n",
    "                    hyponym_name = hyponym.name().lower()\n",
    "                    if hyponym_name not in lemmas:\n",
    "                        lemmas.add(hyponym_name)\n",
    "                        results = add_topics(hyponym_name, word, results, results_n)      \n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "\n",
    "    return list(results)\n",
    "\n",
    "# ссылки\n",
    "special_char = {'<': '%3C', '>': '%3E', '#': '%23', '%': '%25', '{': '%7B', '}': '%7D', '|': '%7C', '\\\\': '%5C', \n",
    "                '^': '%5E', '~': '%7E', '[': '%5B', ']': '%5D', '`': '%60', ';': '%3B', '/': '%2F', '?': '%3F', \n",
    "                ':': '%3A', '@': '%40', '=': '%3D', '&': '%26', '$': '%24', '+': '%2B', '\"': '%22', ' ': '%20'}\n",
    "def right_link(link):\n",
    "    main_link = 'https://digitalcollections.nypl.org/search/index?filters%5Btopic%5D='\n",
    "    for i in special_char:\n",
    "        link = link.replace(i, special_char[i])\n",
    "    return main_link + link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [31/May/2017 05:14:54] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/May/2017 05:14:54] \"GET /static/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/May/2017 05:14:54] \"GET /static/css/manual.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/May/2017 05:14:54] \"GET /static/js/respond.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/May/2017 05:14:54] \"GET /static/js/bootstrap.min.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template, url_for, redirect\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/result/<search>')\n",
    "def result(search):\n",
    "    if len(request.args) > 0:\n",
    "        search = request.args['search']\n",
    "            \n",
    "#     results = [\"Writing, Hieroglyphic\", \"Religion\", \"Egypt\", \"Inscriptions\", \"Papyri, Hieratic\", \"Obelisks\",\n",
    "#                \"Study and teaching\", \"Foreign speakers\", \"Inscriptions, Greek\", \"Herbs\"]\n",
    "\n",
    "    query = analyze_query(search)\n",
    "    results = sim_graph(search)\n",
    "    if len(results) < 10:\n",
    "        results += sim_word2vec(search)\n",
    "    if len(results) < 10:\n",
    "        results += list(sim_wordnet(search))\n",
    "        \n",
    "    new_results = []\n",
    "    for result in results:\n",
    "        new_results.append([result, right_link(result)])\n",
    "    \n",
    "    if new_results != []:\n",
    "        return render_template('result.html', search=search, results=new_results)\n",
    "    \n",
    "    return render_template('no_results.html', search=search)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    if len(request.args) > 0:\n",
    "        search = request.args['search']\n",
    "\n",
    "        return redirect(url_for('result', search=search))\n",
    "    \n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/about/')\n",
    "def about():\n",
    "    return render_template('about.html')\n",
    "\n",
    "@app.route('/help/')\n",
    "def help():\n",
    "    return render_template('help.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
