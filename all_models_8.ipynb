{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import re\n",
    "import treetaggerwrapper\n",
    "from os import path\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "special_char = {'<': '%3C', '>': '%3E', '#': '%23', '%': '%25', '{': '%7B', '}': '%7D', '|': '%7C', '\\\\': '%5C', \n",
    "                '^': '%5E', '~': '%7E', '[': '%5B', ']': '%5D', '`': '%60', ';': '%3B', '/': '%2F', '?': '%3F', \n",
    "                ':': '%3A', '@': '%40', '=': '%3D', '&': '%26', '$': '%24', '+': '%2B', '\"': '%22', ' ': '%20'}\n",
    "def right_link(link):\n",
    "    main_link = 'https://digitalcollections.nypl.org/search/index?filters%5Btopic%5D='\n",
    "    for i in special_char:\n",
    "        link = link.replace(i, special_char[i])\n",
    "    return main_link + link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://digitalcollections.nypl.org/search/index?filters%5Btopic%5D=Clothing%20%26%20dress'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://digitalcollections.nypl.org/search/index?filters%5Btopic%5D=' + right_link('Clothing & dress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "\n",
    "def analyze_query(query):\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    tokens = word_tokenize(query)\n",
    "    punct = ',.()\":;--&?!\\'s'\n",
    "    tokens = [token for token in tokens if token not in punct]\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    new_query = ''\n",
    "    for token in tokens:\n",
    "        lemma = tagger.tag_text(token)[0].split('\\t')[-1]\n",
    "        new_query += lemma + ' '\n",
    "    \n",
    "    return new_query.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_query('houses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.read_gexf('topics.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_topics = json.loads(open('analyzed_topics.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_graph(word, results_n=10):   \n",
    "    found_topics = []\n",
    "    results = []\n",
    "    recs = set()\n",
    "    \n",
    "    for topic in new_topics:\n",
    "        n = re.search('(\\s|^)' + word.lower() + '(\\s|$)', topic)\n",
    "        if n is not None:\n",
    "            for node_1 in new_topics[topic]:\n",
    "                for node_2 in G.neighbors(node_1):\n",
    "                    if node_2 not in recs:\n",
    "                        n = re.search('(\\s|^|\\W)' + word.lower() + '(\\s|$|\\W)', node_2.lower())\n",
    "                        if n is None:\n",
    "                            recs.add(node_2)\n",
    "                            weight = G.edge[node_1][node_2]['weight']\n",
    "                            results.append([node_2, weight])\n",
    "\n",
    "    if results != []:\n",
    "        results = sorted(results, key=lambda x: x[1], reverse=True)[:results_n]\n",
    "        results = [x[0] for x in results]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = json.loads(open('model_in_dic.json', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_word2vec(word, results_n=10):\n",
    "    pos_tags = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB']\n",
    "    results = []\n",
    "\n",
    "    for pos_tag in pos_tags:\n",
    "        word_tagged = word.replace(' ', '::') + '_' + pos_tag\n",
    "        if word_tagged in model:\n",
    "            for sim_word in model[word_tagged]:\n",
    "                sim_word = sim_word.replace('::', '(-| )')\n",
    "                for topic in new_topics:\n",
    "                    n = re.search('(\\s|^)' + sim_word.lower() + '(\\s|$)', topic)\n",
    "                    if n is not None and word not in topic:\n",
    "                        result = new_topics[topic] \n",
    "                        for r in result:\n",
    "                            if r not in results:\n",
    "                                 results.append(r)\n",
    "                            if len(results) >= results_n:\n",
    "                                return results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Festoons', 'festoons']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_word2vec('line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_topics(lemma, word, results, results_n):\n",
    "    for topic in new_topics:\n",
    "        n = re.search('(\\s|^)' + lemma.replace('_', ' ') + '(\\s|$)', topic)\n",
    "        if n is not None:\n",
    "            m = re.search('(\\s|^)' + word.replace('_', ' ') + '(\\s|$)', topic)\n",
    "            if m is None:\n",
    "                for i in new_topics[topic]:\n",
    "                    results.add(i)\n",
    "                if len(results) >= results_n:\n",
    "                    return results\n",
    "    return results\n",
    "\n",
    "def sim_wordnet(word, results_n=10):\n",
    "    results = set()\n",
    "    word = word.replace(' ', '_')\n",
    "    lemmas = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for synonym in syn.lemmas():\n",
    "            synonym_name = synonym.name().lower()\n",
    "            if synonym_name != word and synonym_name not in lemmas:\n",
    "                lemmas.add(synonym_name)\n",
    "                results = add_topics(synonym_name, word, results, results_n)\n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "        \n",
    "    lemmas = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for synonym in syn.lemmas():\n",
    "            if synonym.antonyms():\n",
    "                for antonym in synonym.antonyms():\n",
    "                    antonym_name = antonym.name().lower()\n",
    "                    if antonym_name not in lemmas:\n",
    "                        lemmas.add(antonym_name)\n",
    "                        results = add_topics(antonym_name, word, results, results_n)\n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "    \n",
    "    lemmas = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.hypernyms():\n",
    "            for hyper in syn.hypernyms():\n",
    "                for hypernym in hyper.lemmas():\n",
    "                    hypernym_name = hypernym.name().lower()\n",
    "                    if hypernym_name not in lemmas:\n",
    "                        lemmas.add(hypernym_name)\n",
    "                        results = add_topics(hypernym_name, word, results, results_n)\n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "        \n",
    "    lemmas = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.hyponyms():\n",
    "            for hypo in syn.hyponyms():\n",
    "                for hyponym in hypo.lemmas():\n",
    "                    hyponym_name = hyponym.name().lower()\n",
    "                    if hyponym_name not in lemmas:\n",
    "                        lemmas.add(hyponym_name)\n",
    "                        results = add_topics(hyponym_name, word, results, results_n)      \n",
    "        if len(results) >= results_n:\n",
    "            return results\n",
    "\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theater\n",
      "american\n",
      "hat\n",
      "biography\n",
      "build\n",
      "man\n",
      "dance\n",
      "russian\n",
      "design\n",
      "punishment\n",
      "mythology\n",
      "work\n",
      "sculpture\n",
      "horseback\n",
      "girl\n",
      "china\n",
      "sword\n",
      "radio\n",
      "bath\n",
      "piano\n"
     ]
    }
   ],
   "source": [
    "arr = ['theater', 'american', 'hat', 'biography', 'build', 'man', 'dance', 'russian', 'design', 'punishment', 'mythology',\n",
    "       'work', 'sculpture', 'horseback', 'girl', 'china', 'sword', 'radio', 'bath', 'piano']\n",
    "exp = ''\n",
    "\n",
    "for i in arr:\n",
    "    print(i)\n",
    "    exp += i\n",
    "    results_graph = '\\n'.join(sim_graph(i))\n",
    "    results_word2vec = '\\n'.join(sim_word2vec(i))\n",
    "    results_wordnet = '\\n'.join(sim_wordnet(i))\n",
    "    exp += '\\n' + '-------------1-------------' + '\\n' + results_graph \n",
    "    exp += '\\n' + '-------------2-------------' + '\\n' + results_word2vec\n",
    "    exp += '\\n' + '-------------3-------------' + '\\n' + results_wordnet\n",
    "    exp += '\\n' + '------------------------------------------------------' + '\\n'\n",
    "\n",
    "with open('evaluation_sample.txt', 'w', encoding='utf-8') as outfile:\n",
    "    outfile.write(exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
